{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'simple.txt'}, page_content='Learn more about how we live our values at GitLab\\nCREDIT\\nGitLab‚Äôs six core values are ü§ù Collaboration, üìà Results for Customers, ‚è±Ô∏è Efficiency, üåê Diversity, Inclusion & Belonging, üë£ Iteration, and üëÅÔ∏è Transparency, and together they spell the CREDIT we give each other by assuming good intent. We react to them with values emoji and they are made actionable below.\\n\\nAbout our values\\nCollaboration Results Efficiency Diversity, Inclusion & Belonging Iteration Transparency\\n\\nWe take inspiration from other companies, and we always go for the boring solutions. Our CEO, Sid Sijbrandij, has shared the origin of each of the CREDIT values, but just like the rest of our work, we continually adjust our values and strive to make them better. GitLab values are a living document. In many instances, they have been documented, refined, and revised based on lessons learned (and scars earned) in the course of doing business.\\n\\nWe used to have more values, but it was difficult to remember them all. In response, we condensed them, created an acronym (CREDIT), and listed operating principles to guide behavior.\\n\\nEveryone is welcome to suggest improvements. Please assign MRs to update these values to our CEO Sid and @mention him in Slack if you work at GitLab Inc. or on Twitter if you don‚Äôt.\\n\\n\\nDriving Results with CREDIT from GitLab on Vimeo.\\n\\nü§ù Collaboration\\nTo achieve results, team members must work together effectively. At GitLab, helping others is a priority, even when it is not immediately related to the goals that you are trying to achieve. Similarly, you can rely on others for help and advice‚Äîin fact, you‚Äôre expected to do so. Anyone can chime in on any subject, including people who don‚Äôt work at GitLab. The person who‚Äôs responsible for the work decides how to do it, but they should always take each suggestion seriously and try to respond and explain why it may or may not have been implemented.\\n\\nKindness\\nWe value caring for others. Demonstrating we care for people provides an effective framework for challenging directly and delivering feedback. Kindness doesn‚Äôt mean holding back on feedback or avoiding disagreements, these are crucial to professional growth and getting results for customers. Kindness means you make a separation between the work and the person, you can criticize someone‚Äôs work but still be respectful to the person. Give as much positive feedback as you can, and do it in a public way.\\n\\nShare\\nThere are aspects of GitLab culture, such as intentional transparency, that are unintuitive to outsiders and new team members. Be willing to invest in people and engage in open dialogue. For example, consider making private issues public wherever possible so that we can all learn from the experience. Don‚Äôt be afraid of judgement or scrutiny when sharing publicly, we all understand it‚Äôs impossible to know everything.\\n\\nEveryone can remind anyone in the company about our values. If there is a disagreement about the interpretations, the discussion can be escalated to more people within the company without repercussions.\\n\\nShare problems you run into, ask for help, be forthcoming with information and speak up.\\n\\nNegative feedback is 1-1\\nGive negative feedback in the smallest setting possible. One-on-one video calls are preferred.\\n\\nNegative feedback is distinct from negativity and disagreement. If there is no direct feedback involved, strive to discuss disagreement in a public channel, respectfully and transparently.\\n\\nIn a GitLab Unfiltered interview on values, GitLab co-founder and CEO Sid Sijbrandij offers the following context.\\n\\nWe deal with negative all the time at GitLab. If it‚Äôs not a problem, then why are we discussing it? We deal with negativity a lot, and that‚Äôs also part of our ambition.\\n\\nIf you want to get better, you talk about what you can improve. We‚Äôre allowed to publicly discuss negative things; we‚Äôre not allowed to give negative feedback in a large setting if it could be feasibly administered in a smaller setting.\\n\\nNegative feedback can be given in a group setting if it‚Äôs to someone higher in the management chain. This shows that no one is above feedback. GitLab co-founder and CEO Sid Sijbrandij and former CTO Eric Johnson discuss this in this GitLab Unfiltered video.\\n\\nProvide feedback in a timely manner\\nWe want to solve problems while they are small. If you are unhappy with anything (your duties, your colleague, your boss, your salary, your location, your computer), please voice your concerns rather than keeping them to yourself. If you need to escalate beyond your manager, you could consider speaking to your skip-level, a more senior person, or a people business partner.\\n\\nSay thanks\\nRecognize the people that helped you publicly, for example in our #thanks chat channel.\\n\\nWhen publicly thanking, it‚Äôs important to recognize the following:\\n\\nShowing thanks in as large a setting as possible (company-wide) at a company as large as ours is the exception instead of the norm, it takes some getting used to.\\nBeing thanked at the company level for what you view as a relatively small or minuscule contribution can feel awkward.\\nThanking a person in #thanks should be done sincerely and summarize why you are thankful so the person on the receiving end can easily understand why they are being thanked. Even while assuming positive intent, not all folks are comfortable with public praise. Help this person understand how they went above and beyond and why you felt it was important for the team member to be recognized.\\nThere are a number of good ways and places to say thanks. We shouldn‚Äôt limit saying thanks to just the #thanks channel.\\nGive feedback effectively\\nGiving feedback is challenging, but it‚Äôs important to deliver it effectively. When providing feedback, always make it about the work itself; focus on the business impact and not the person. Make sure to provide at least one clear and recent example. If a person is going through a hard time in their personal life, then take that into account. An example of giving positive feedback is our thanks chat channel. For managers, it‚Äôs important to realize that team members react to a negative incident with their managers six times more strongly than they do to a positive one. Keeping that in mind, if an error is so inconsequential that the value gained from providing criticism is low, it might make sense to keep that feedback to yourself. In the situations where negative feedback must be given, focus on the purpose for that feedback: to improve the team member‚Äôs performance going forward. Give recognition generously, in the open, and often to generate more engagement from your team.\\n\\nGet to know each other\\nWe use a lot of text-based communication, and if you know the person behind the text, it will be easier to prevent conflicts. So we encourage people to get to know each other on a personal level through informal communication, for example, virtual coffee chats, and during GitLab Contribute.\\n\\nReach across company departments\\nWhile it‚Äôs wise to seek advice from experts within your function, we encourage GitLab team members to do the same across departments. This enables the company to iterate more quickly, embrace the understanding that everyone can contribute and include more diverse perspectives when possible.\\n\\nDon‚Äôt pull rank\\nIf you have to remind someone of the position you have in the company, you‚Äôre doing something wrong. People already know our decision-making process. Explain why you‚Äôre making the decision, and respect everyone irrespective of their function. This includes using the rank of another person - including the CEO - to sell an idea or decision.\\n\\nAssume positive intent\\nWe naturally have a double standard when it comes to the actions of others. We blame circumstances for our own mistakes, but individuals for theirs. This double standard is called the Fundamental Attribution Error. In order to mitigate this bias, you should always assume positive intent in your interactions with others, respecting their expertise and giving them grace in the face of what you might perceive as mistakes.\\n\\nWhen disagreeing, folks sometimes argue against the weakest points of an argument, or an imaginary argument (e.g. ‚Äústraw man‚Äù). Assume the points are presented in good faith, and instead try to argue against the strongest version of your opponent‚Äôs position. We call this arguing against a ‚Äústeel‚Äù position, instead of a ‚Äústraw‚Äù one. This concept is borrowed from argue the ‚Äústeel man‚Äù technique.\\n\\nA ‚Äústeel‚Äù position should be against the absolute most effective version of your opponent‚Äôs position ‚Äî potentially even more compelling than the one they presented. A good ‚Äústeel‚Äù position is one where the other person feels you‚Äôve represented their position well, even if they still disagree with your assumptions or conclusion.\\n\\nAddress behavior, but don‚Äôt label people\\nThere is a lot of good in this article about not wanting jerks on our team, but we believe that jerk is a label for behavior rather than an inherent classification of a person. We avoid classifications.\\n\\nSay sorry\\nIf you made a mistake, apologize as soon as possible. Saying sorry is not a sign of weakness but one of strength. The people that do the most work will likely make the most mistakes. Additionally, when we share our mistakes and bring attention to them, others can learn from us, and the same mistake is less likely to be repeated by someone else. Mistakes can include when you have not been kind to someone. In order to reinforce our values, it is important, and takes more courage, to apologize publicly when you have been unkind publicly (e.g., when you have said something unkind or unprofessional to an individual or group in a Slack channel).\\n\\nNo ego\\nDon‚Äôt defend a point to win an argument or double-down on a mistake. You are not your work; you don‚Äôt have to defend your point. You do have to search for the right answer with help from others.\\n\\nIn a GitLab Unfiltered interview, GitLab Head of Remote Darren M. adds context on this operating principle.\\n\\nIn many organizations, there‚Äôs a subtle, low-level, persistent pressure to continually prove your worth. And I believe that this fuels imposter syndrome and wreaks havoc on mental health.\\n\\nWhat‚Äôs so troubling to me is how often perception is reality. In other words, those who have mastered the art of being perceived as elite reap benefits, though this has nothing to do with actual results.\\n\\nAt GitLab, ‚Äúno ego‚Äù means that we foster and support an environment where results matter, and you‚Äôre given agency to approach your work in the way that makes sense to you. Instead of judging people for not approaching work in an agreed-upon way, ‚Äúno ego‚Äù encourages people to glean inspiration from watching others approach work in new and different ways.\\n\\nBeing no ego is a standard we hold ourselves as people to but is not one that applies to GitLab as a company or product. We want to celebrate and highlight GitLab‚Äôs accomplishments, including being one of the largest all-remote companies. This doesn‚Äôt mean we don‚Äôt recognize our mistakes, including how we handled telemetry.\\n\\nSee others succeed\\nA candidate who has talked to a lot of people inside GitLab said that, compared to other companies, one thing stood out the most: everyone here mentioned wanting to see each other succeed.\\n\\nDon‚Äôt let each other fail\\nKeep an eye out for others who may be struggling or stuck. If you see someone who needs help, reach out and assist. This might involve offering to pair program or setting up a sync brainstorming session. The goal is to connect them with someone else who can provide expertise or assistance. We are a team, so we succeed and shine together by supporting each other!\\n\\nPeople are not their work\\nAlways make suggestions about examples of work, not the person. Say ‚ÄúYou didn‚Äôt respond to my feedback about the design‚Äù instead of ‚ÄúYou never listen‚Äù. And, when receiving feedback, keep in mind that feedback is the best way to improve, and that others giving you feedback want to see you succeed.\\n\\nDo it yourself\\nOur collaboration value is about helping each other when we have questions, need critique, or need help. No need to brainstorm, wait for consensus, or do with two what you can do yourself. The Bolt Handbook refers to this as the Founder Mentality, where all team members should approach the problem as if they own the company.\\n\\nBlameless problem solving\\nInvestigate mistakes in a way that focuses on the situational aspects of a failure‚Äôs mechanism and the decision-making process that led to the failure, rather than cast blame on a person or team. We hold blameless root cause analyses and retrospectives for stakeholders to speak up without fear of punishment or retribution.\\n\\nShort toes\\nPeople joining the company frequently say, ‚ÄúI don‚Äôt want to step on anyone‚Äôs toes.‚Äù At GitLab, we should be more accepting of people taking initiative in trying to improve things. As companies grow, their speed of decision-making goes down since there are more people involved. We should counteract that by having short toes and feeling comfortable letting others contribute to our domain. For example, pointed, respectful feedback to a proposal by GitLab‚Äôs CEO led to his own merge request being closed. However, it is not required to respond to comments.\\n\\nIt‚Äôs impossible to know everything\\nWe know we must rely on others for the expertise they have that we don‚Äôt. It‚Äôs OK to admit you don‚Äôt know something and to ask for help, even if doing so makes you feel vulnerable. It is never too late to ask a question, and by doing so, you can get the information you need to produce results and to strengthen your own skills as well as GitLab as a whole. After your question is answered, please document the answer so that it can be shared.\\n\\nDon‚Äôt display surprise when people say they don‚Äôt know something, as it is important that everyone feels comfortable saying ‚ÄúI don‚Äôt know‚Äù and ‚ÄúI don‚Äôt understand.‚Äù (As inspired by Recurse.)\\n\\nCollaboration is not consensus\\nWhen collaborating, it is always important to stay above radar and work transparently, but collaboration is not consensus and disagreement is part of collaboration. You don‚Äôt need to ask people for their input, and they shouldn‚Äôt ask you ‚ÄúWhy didn‚Äôt you ask me?‚Äù You don‚Äôt have to wait for people to provide input, if you did ask them. You don‚Äôt need to have everyone agreeing to the same thing - they can disagree, commit, and disagree. Two-way doors decisions can be reversed as part of disagree, commit, and disagree, while one-way door decisions benefit from more input. Recognize these reversible two-way door decisions for when less input is required to iterate faster. We believe in permissionless innovation‚Äîyou don‚Äôt need to involve people, but everyone can contribute. This is core to how we iterate, since we want smaller teams moving quickly rather than large teams achieving consensus slowly.\\n\\nCollaboration Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate collaboration when we take action to help others and include other‚Äôs (both internal and external) input (both help and feedback) to achieve the best possible outcome.\\n\\nGitLab Job Grade\\tDemonstrates Collaboration Competency by‚Ä¶\\tKnowledge Assessment\\n5\\tDevelops collaboration skills by learning from other team members\\tKnowledge Assessment for Individual Contributors\\n6\\tGrows collaboration skills by using different types of communication; files issues appropriately, asks in the right Slack channels and uses the right labels.\\n7\\tModels collaborative behavior for fellow team members and others within the group.\\n8\\tCoaches team members on how to collaborate more effectively and pointing team members to the right channels to collaborate.\\tKnowledge Assessment for People Leaders\\n9\\tFosters collaborative decision making and problem solving across the departments.\\n10\\tDrives team collaboration across divisions/departments, silos, and division boundaries.\\n11\\tDevelops networks and builds partnerships, engages in cross-functional activities; collaborates across boundaries, and finds common ground with a widening range of stakeholders. Utilizes contacts to build and strengthen internal support base\\n12\\tLeads collaboration and teamwork in daily routines, prioritizing interactions, information sharing, and real time decision making across divisions/departments. Encourages greater cross-functional collaboration among e-team leaders.\\n14\\tChampions collaboration and teamwork into daily routines, prioritizing interactions, information sharing, and real time decision making across divisions/departments. Champions cross-functional collaboration among e-team leaders and GitLab.\\nüìà Results for Customers\\nWe exist to help our customers achieve more. Everything we do should be in service of making our customers successful with GitLab. Results for Customers is at the top of our values hierarchy, as our customers achieving results drives overall business performance that enables everything else.\\n\\nThe Results for Customers value is displayed through the following operating principles:\\n\\nSet Ambitious & Measurable goals\\nWhile we iterate with small changes, we strive for large, ambitious results. We have an ambitious mission and vision, and we aim to be the best in the world across all our functions. Setting ambitious, measurable goals enables us to best deliver customer results. We agree in writing on measurable goals. Within the company we use OKRs to stay accountable. We have and report against KPIs with guiding targets.\\n\\nUnderstand our customers\\nAll GitLab team members should understand our customers‚Äô needs, issues, and value propositions. We understand how they use GitLab and what they need from a platform in order to meet their goals. Internally facing teams consider the impact of their work as it pertains indirectly to GitLab‚Äôs customers.\\n\\nWe better understand customers and their needs through:\\n\\nReviewing public facing GitLab issues from our customers and users\\nDogfooding our product to understand the user experience\\nReading customer stories from Marketing and Sales\\nAttending Customer fireside chats\\nLearning feedback from our customers and users on product features and roadmap\\nCo-create\\nWe create together with our customers. There is an open dialogue between GitLab and our customers so that we can better identify what they need. As a result of building a solution for them, we can also bring that solution to the world.\\n\\nKeep end users in sight\\nOur focus is to increase customer results. At GitLab, one way to drive customer results is through platform enhancements that drive the most value for direct users. This requires being aware of the Concur effect.\\n\\nArvind Narayanan, a Princeton Professor, described his frustration with Blackboard in a viral Tweet:\\n\\nIt has every feature ever dreamed up. But like anything designed by a committee, the interface is incoherent and any task requires at least fifteen clicks (and that‚Äôs if you even remember the correct sequence the first time).\\n\\nSoftware companies can be breathtakingly clueless when there‚Äôs a layer of indirection between them and their users. Everyone who‚Äôs suffered through Blackboard will have the same reaction to this: try having less functionality!\\n\\nRyan Falor followed up on Narayanan‚Äôs tweet with his definition of the Concur Effect:\\n\\ndecision makers are not direct users\\nfeatures are overwhelming and disjointed\\nuser experience gets worse over time\\nSee the Hacker News discussion for a specific UX example.\\n\\nAt GitLab, we want to drive customer results through focusing on platform enhancements that drive the most value for direct users.\\n\\nCustomer results are more important than:\\n\\nWhat we plan to make. If we focus only on our own plans, we would have only GitLab.com and no self-managed delivery of GitLab. This does not mean that we will agree to every feature request, but we won‚Äôt let existing plans be an obstacle to working on what will drive the most customer value.\\nLarge customer requests. Catering to requests from large customers leads to the innovator‚Äôs dilemma, we need to also focus on results for small and future customers.\\nOur existing scope. For example, when customers asked for better integrations and complained about integration costs and effort, we responded by expanding our scope to create a single application for the DevOps lifecycle.\\nOur assumptions. Every company works differently, so we can‚Äôt assume that what works well for us will support our customers‚Äô needs. When we have an idea, we must directly validate our assumptions with multiple customers to ensure we create scalable, highly relevant solutions.\\nWhat we control. We should take responsibility for what the customer experiences, even when it isn‚Äôt entirely in our control. We aim to treat every customer-managed instance downtime as a $1M a day problem.\\nMeasure impact, not activity\\nWe care about what you achieve: the code you shipped, the needle you moved, the user you made happy, and the team member you helped. Someone who took the afternoon off shouldn‚Äôt feel like they did something wrong, unless it negatively impacted a goal or result they were responsible for. You don‚Äôt have to defend how you spend your day if you are performing and delivering against expectations. We trust team members to do the right thing instead of having rigid rules. We trust team members to show up and do their best work. Do not incite competition by proclaiming how many hours you worked yesterday. If you are working too many hours, talk to your manager to discuss solutions.\\n\\nDogfooding\\nWe use our own product in the way our users do to surface improvements that will lead to better customer results. GitLab is a DevSecOps Platform that can be used by people throughout the business. This is how we use it within GitLab. For example, we use our OKR functionality company-wide to inform product enhancements and for team members to have a great understanding of the customer experience. We also dogfood in the following ways:\\n\\nOur development organization uses GitLab.com to manage the DevOps lifecycle of GitLab itself.\\nAll team members use GitLab to collaborate on this handbook.\\nWe capture content and processes in Git repos and manage them with GitLab.\\nWhen something breaks, doesn‚Äôt work well, or needs improvement, we are more likely to notice it internally and address it before it impacts our larger community.\\n\\nGive agency\\nWe give people agency to focus on what they think is most beneficial. If a meeting doesn‚Äôt seem interesting and someone‚Äôs active participation is not critical to the outcome of the meeting, they can always opt to not attend, or during a video call they can work on other things if they want. Staying in the call may still make sense even if you are working on other tasks, so other peers can ping you and get fast answers when needed. This is particularly useful in multi-purpose meetings where you may be involved for just a few minutes.\\n\\nChallenger mindset\\nChallenging the status quo can lead to remarkable results - we must never stop. A challenger mindset requires that we continually ask ourselves bold, difficult questions about our business the problems we solve while resisting complacency. To succeed we must innovate and delight our customers with the value of the products we build. A challenger mindset requires a relentless pursuit of excellence - we must be tenacious. Each win for our customers builds reputational capital we can use to earn the trust of prospects in a competitive market. While competition is a feature of capitalism internally as GitLab team members we must focus our efforts inwardly on achieving our very best results for customers to win market share.\\n\\nGrowth mindset\\nYou don‚Äôt always get results and this will lead to criticism from yourself and/or others. We believe our talents can be developed through hard work, targeted training, learning from others, on-the-job experience, and receiving input from others. It is in our DNA as a company and individuals to look for opportunity, stay humble, and never settle. We try to hire people based on their trajectory, not their pedigree. We also strive to foster a culture of curiosity and continuous learning where team members are provided and proactively seek out opportunities to grow themselves and their careers. We believe that with the right expectations and direction, people can grow to take on new challenges and surpass expectations.\\n\\nCross-functional optimization\\nOur definition of cross-functional optimization is that you do what is best for the organization as a whole. Don‚Äôt optimize for the goals of your team when it negatively impacts the goals of other teams, our users, and/or the company. Those goals are also your problem and your job. For example, you may have set a non-urgent functional milestone that is supposed to land at the end of the quarter. If delivering within the last week requires engagement from the GTM teams, the right decision may be to push your own team‚Äôs target by a week to reduce the ask for the GTM team as the GTM focuses on meeting its revenue objectives.\\n\\nIn the context of collaboration, if anyone is blocked by you on a question, your approval, or a merge request review, you should prioritize unblocking them, either directly or through helping them find someone else who can.\\n\\nEmbrace Tenacity\\nWe refer to this as ‚Äúpersistence of purpose‚Äù. As talked about in The Influence Blog, tenacity is the ability to display commitment to what you believe in. You keep picking yourself up, dusting yourself off, and quickly get going again having learned a little more. We value the ability to maintain focus and motivation when work is tough and asking for help when needed.\\n\\nHave Ownership & Accountability\\nWe expect team members to complete tasks that they are assigned. You are responsible for executing with attention to detail, connecting the dots across the organization and anticipating and solving problems. As an owner, you are responsible for overcoming challenges, not suppliers or other team members. Take initiative and proactively inform stakeholders when there is something you might not be able to solve.\\n\\nSense of urgency\\nTime gained or lost has compounding effects. Try to get the results as fast as possible, but without compromising our other values and ways we communicate, so the compounding of results can begin and we can focus on the next improvement.\\n\\nOperate with a bias for action\\nIt‚Äôs important that we keep our focus on action, and don‚Äôt fall into the trap of analysis paralysis or sticking to a slow, quiet path without risk. Decisions should be thoughtful, but delivering fast results requires the fearless acceptance of occasionally making mistakes; our bias for action also allows us to course correct quickly. Try to get results as fast as possible, but without compromising our other values and ways of working\\n\\nDisagree, commit, and advocate\\nWhen a decision is in place, we expect people to commit to executing it. Any past decisions and guidelines are open to questioning as long as you act in accordance with them until they are changed. This is a common principle. Every decision can be changed; our best decision was one that changed an earlier one. In a manager-report relationship, usually the report is the Directly Responsible Individuals (DRI). The manager may disagree with the final decision, but they still commit to the decision of the DRI.\\n\\nIn a group setting, participants may disagree with a proposal but not articulate their views for one reason or another. Sometimes, many or all individuals may disagree yet choose not to speak up, because no one believes they would get agreement from the group. As a result, everyone loses out on their feedback. Dissent is expression of that disagreement. However, it can be difficult and even socially expensive. Expression of feedback is a way for everyone to grow and learn, and is based on facts rather than opinions. Share your perspective, rather than agreeing simply to avoid conflict or to go along with everyone else.\\n\\nWhen you want to reopen the conversation on something, show that your argument is informed by previous conversations and assume the decision was made with the best intent. You have to achieve results on every decision while it stands, even when you are trying to have it changed. You should communicate with the DRI who can change the decision instead of someone who can‚Äôt.\\n\\nEscalate to unblock\\nIf there is a disagreement and you can‚Äôt move forward because of it, agree to escalate and escalate to one or both of your managers. Early escalation, delivered with context of the challenge, enables managers to function as an unblocker.\\n\\nResults Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate results when we do what we promised to each other, customers, users, and investors.\\n\\nGitLab Job Grade\\tDemonstrates Results Competency by‚Ä¶\\tKnowledge Assessment\\n5\\tDevelops the skills needed to commit and execute on agreed actions.\\tKnowledge Assessment for Individual Contributors\\n6\\tApplies commitment to results and demonstrates ability to execute on agreed actions.\\n7\\tModels a sense of urgency and commitment to deliver results.\\n8\\tCoaches team members to collaborate and work iteratively towards results with the focus on the outcome and not hours worked.\\tKnowledge Assessment for People Leaders\\n9\\tFosters a culture of ownership of personal performance.\\n10\\tDrives efficient execution of results ensuring collaboration between team members.\\n11\\tDevelops quarterly OKR\\'s ensuring the performance and results of one or more teams.\\n12\\tLeads the achievement of results while driving the continued alignment to our values of collaboration, efficiency, diversity, iteration and transparency.\\nEVP/CXO\\tLeads the achievement of results while driving the continued alignment to our values of collaboration, efficiency, diversity, iteration and transparency.\\n‚è±Ô∏è Efficiency\\nAt GitLab, efficiency means producing results without wasting materials, time, or energy. We optimize solutions globally for the broader GitLab community over one person or a small group. Focus on efficiency should be global in nature, not just local to a given function. Global efficiency could include efficiency with customers, candidates, and contributors as well. It is easy to prioritize consistency over efficiency because consistency is often more efficient initially and makes managing processes more efficient. We should slow down when optimizing for consistency. Taking a company-wide lens when evaluating changes will help ensure that new processes will improve efficiency for GitLab as a whole and be the best decision for the company as a whole.\\n\\nWhen we work internally with other team members, we leverage GitLab‚Äôs unique working practices and operating principles to achieve top efficiency. We do not expect people outside of GitLab to conform to GitLab‚Äôs ways of working, and we will make accommodations to work effectively with them. For example, we may collaborate heavily in-person and not default to async communications.\\n\\nOnly Healthy Constraints\\nMost companies regress to the mean and slow down over time. While some changes are required as a company grows and matures, not all change is inevitable or should be allowed to passively happen. As GitLab grows, we are conscious of how we operate and how it enables our ability to continue to operate with the agility of a startup. We try to limit ourselves to healthy constraints.\\n\\nWrite things down\\nWe document everything: in the handbook, in meeting notes, in issues. We do that because ‚Äúthe faintest pencil is better than the sharpest memory.‚Äù It is far more efficient to read a document at your convenience than to have to ask and explain. Having something in version control also lets everyone contribute suggestions to improve it.\\n\\nBoring solutions\\nUse the simplest and most boring solution for a problem, and remember that ‚Äúboring‚Äù should not be conflated with ‚Äúbad‚Äù or ‚Äútechnical debt.‚Äù The speed of innovation for our organization and product is constrained by the total complexity we have added so far, so every little reduction in complexity helps. Don‚Äôt pick an interesting technology just to make your work more fun; using established, popular tech will ensure a more stable and more familiar experience for you and other contributors.\\n\\nMake a conscious effort to recognize the constraints of others within the team. For example, sales is hard because you are dependent on another organization, and development is hard because you have to preserve the ability to quickly improve the product in the future.\\n\\nSelf-service and self-learning\\nTeam members should first search for their own answers and, if an answer is not readily found or the answer is not clear, ask in public as we all should have a low level of shame. Write down any new information discovered and pay it forward so that those coming after will have better efficiency built on top of practicing collaboration, inclusion, and documenting the results.\\n\\nTeam members have more room to grow themselves when they are able to self-service and self-learn.\\n\\nEfficiency for the right group\\nOptimize solutions globally for the broader GitLab community. As an example, it may be best to discard a renewal process that requires thousands of customers to each spend two hours in favor of one that only takes sixty seconds, even when it may make a monthly report less efficient internally! In a decision, ask yourself ‚ÄúFor whom does this need to be most efficient?‚Äù Quite often, the answer may be your users, contributors, customers, or team members that are dependent upon your decision.\\n\\nBe respectful of others‚Äô time\\nConsider the time investment you are asking others to make with meetings and a permission process. Try to avoid meetings, and if one is necessary, try to make attendance optional for as many people as possible. Any meeting should have an agenda linked from the invite, and you should document the outcome. Instead of having people ask permission, trust their judgment and offer a consultation process if they have questions.\\n\\nSpend company money like it‚Äôs your own\\nEvery dollar we spend will have to be earned back. Be as frugal with company money as you are with your own. In saying this, we ask team members to weigh the cost of purchases against the value that they will bring to the company.\\n\\nConsider the degree to which a purchase increases your ability to better accomplish your work and achieve business results relative to cost. Lowering overhead reduces the cost to operate the business and lets us shift spend toward other priority areas.\\n\\nWe have guidelines around this operating principle to help team members better understand our expensing process and expectations.\\n\\nFrugality\\nAmazon states it best with: ‚ÄúAccomplish more with less. Constraints breed resourcefulness, self-sufficiency, and invention. There are no extra points for growing headcount, budget size, or fixed expense.‚Äù\\n\\nShort verbal answers\\nGive short answers to verbal questions so the other party has the opportunity to ask more or move on.\\n\\nKeep broadcasts short\\nKeep one-to-many written communication short, as mentioned in this HBR study: ‚ÄúA majority say that what they read is frequently ineffective because it‚Äôs too long, poorly organized, unclear, filled with jargon, and imprecise.‚Äù\\n\\nManagers of one\\nWe want each team member to be a manager of one who doesn‚Äôt need daily check-ins to achieve their goals. Team members are given the freedom to own projects and initiatives and are trusted to see them through to a successful end.\\n\\nWhen team members are managers of one they can have an increased work/life balance, because they are more empowered to make decisions around how they allocate their time throughout each day.\\n\\nFreedom and responsibility over rigidity\\nWhen possible, we give people the responsibility to make a decision and hold them accountable for that, instead of imposing rules and approval processes. You should have clear objectives and the freedom to work on them as you see fit. Freedom and responsibility are more efficient than rigidly following a process, or creating interdependencies, because they enable faster decision velocity and higher rates of iteration.\\n\\nWhen team members have freedom and responsibility over rigidity, they have more room to help others.\\n\\nAccept mistakes\\nNot every problem should lead to a new process to prevent them. Additional processes make all actions more inefficient; a mistake only affects one. Once you have accepted the mistake, learn from it. When team members are free to accept mistakes, they can take more calculated risks.\\n\\nMove fast by shipping the minimal valuable change\\nWe value constant improvement by iterating quickly, month after month. If a task is not the smallest viable and valuable thing, cut the scope.\\n\\nEmbrace change\\nAdoption of features, user requirements, and the competitive landscape change frequently and rapidly. The most successful companies adapt their roadmap and their organization quickly to keep pace. One of the things that makes this challenging is the impact on our team. People may need to change teams, subject matter, or even who manages them. This can rightly feel disruptive. If we coach ourselves to embrace the positive aspects of change, such as increased opportunity and new things to learn, we can move faster as a company and increase our odds of success. It is important to hold management accountable for being deliberate.\\n\\nEfficiency Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate efficiency when we work on the right things, not doing more than needed, and not duplicating work.\\n\\nGitLab Job Grade\\tDemonstrates Efficiency Competency by‚Ä¶\\tKnowledge Assessment\\n5\\tDevelops an understanding of being a manager of 1 by taking responsibility for your own tasks and delivering on commitments. Brings up ideas for process improvements to 1:1s. Learns to write everything down as it is far more efficient to read a document at your convenience than to have to ask and explain.\\tKnowledge Assessment for Individual Contributors\\n6\\tDevelops a deeper understanding of efficiency and actively identifies process inefficiencies within the team. Seeks out ways to be more effective in their role, while also starting to mentor others in ways to work efficiently.\\n7\\tModels a culture of efficiency within the team where people make good, timely decisions using available data and assessing multiple alternatives. Models using boring solutions for increasing the speed of innovation for our organization and product.\\n8\\tTakes ownership of own team process inefficiencies, implements cross team efforts in ensuring things are running smoothly. Implements a way of working in the team where team members first search for their own answers and, if an answer is not readily found or the answer is not clear, ask in public as we all should have a low level of shame.\\tKnowledge Assessment for People Leaders\\n9\\tTakes ownership of group level process inefficiencies and guides cross sub-departments in ensuring things are running smoothly. Fosters a culture in the sub-departments where you respect others\\' time and promote self-service and self-learning.\\n10\\tDrives the framework of frugality on a department level and owns departments efforts in ensuring things are running smoothly. Drives efficient resolution of highly complex or unusual business problems that impact the department / team. Holds their managers and peers accountable for upholding this value.\\n11\\tDevelops the framework and strategy of frugality cross division resulting in efforts ensuring things are running smoothly. Develops leaders to action on division/department/team inefficiencies. Hold their management teams accountable for upholding this value.\\n12\\tLeads with efficiency across the company. Ensures efficient resource allocation decisions across the company. Leads across company strategy and policy improvements that move the business towards more efficiency. They hold their senior management and the e-group accountable for upholding this value.\\nEVP/CXO\\tChampions GitLab\\'s strategy for efficiency internally and externally. Constantly looking for efficiency improvements cross company and holding other e-group members accountable for upholding efficiency too. They are comfortable leading through frugality and accepting of mistakes.\\nüåê Diversity, Inclusion & Belonging\\nDiversity, inclusion and belonging are fundamental to the success of GitLab. We aim to make a significant impact in our efforts to foster an environment where everyone can thrive. We are designing a multidimensional approach to ensure that GitLab is a place where people from every background and circumstance feel like they belong and can contribute. We actively chose to build and institutionalize a culture that is inclusive and supports all team members equally in the process of achieving their professional goals. We hire globally and encourage hiring in a diverse set of countries. We work to make everyone feel welcome and to increase the participation of underrepresented minorities and nationalities in our community and company. For example, we celebrate our sponsorship of diversity, inclusion & belonging events.\\n\\nBias towards asynchronous communication\\nTake initiative to operate asynchronously whenever possible. This shows care and consideration for those who may not be in the same time zone, are traveling outside of their usual time zone, or are structuring their day around pressing commitments at home or in their community.\\n\\nThis is demonstrated by communicating recordings of meetings, using GitLab Issues and Merge Requests rather than texts, calls, or Slack messages, and being sensitive to local holidays and vacation statuses. Encourage others to default to documentation rather than pressuring others to be online outside of their working hours.\\n\\nEmbracing uncomfortable ideas and conversations\\nPart of embracing diversity is a willingness to embrace often uncomfortable conversations and situations. This concept is also at the core of inclusion and helping to eliminate the problems that are faced by certain GitLab team members who may not be in the majority.\\n\\nWe believe that being willing to embrace discomfort is the path forward to a safe, balanced and inclusive work place for all. Challenge yourself, challenge your own pre-set notions and ideas about different cultures or things you don‚Äôt understand. When we are willing to embrace being uncomfortable, we can focus on actually fixing the issues at hand rather than simply ‚Äúappearing to care‚Äù.\\n\\nUnderstanding the impact of microaggressions\\nMicroaggressions are much more than merely rude or insensitive comments. They can wear people down by slowly chipping away their sense of belonging/safety/inclusion over time. What is a microaggression?\\n\\n‚ÄúThe everyday slights, indignities, put downs and insults that people of color, women, LGBT populations or those who are marginalized experiences in their day-to-day interactions with people.‚Äù - Derald W. Sue\\n\\nAt GitLab we believe that everyone is entitled to a safe working space where they can express who they are and participate in conversations without worry of being spoken to in a harmful way, given that we want to encourage everyone to be mindful of what is a microaggression and be mindful of their potential impact.\\n\\nSeek diverse perspectives\\nWe believe that team members seeking feedback from a diverse group of team members, inside and outside of their group or function, leads to better decisions and a greater sense of team member belonging. For more guidance on how we define Diversity, please refer to GitLab‚Äôs definition of Diversity, Inclusion & Belonging. Feedback from a more heterogenous group often leads to better business outcomes as we incorporate diverse perspectives and uncover unconscious bias.\\n\\nAn example of this operating principle in action showcases the value of actively seeking diverse perspectives. The term ‚ÄúBrag Document‚Äù was used to describe when individuals documented their accomplishments. Documenting accomplishments is critical to team member development. However, team members had the psychological safety to raise the question of whether or not the title of the document made some feel uncomfortable. In an effort to seek a diverse perspective, a survey was conducted in one of the Team Member and Advocacy Resource Group (TMRG) channels. The poll results showed that 100% of those polled preferred a different title and the title was changed.\\n\\nMake family feel welcome\\nOne of the unique elements to an all-remote culture is the ability to visit a person‚Äôs home while collaborating. If the tenor of the meeting allows, feel welcome to invite your family members or pets to drop by and greet your colleagues. Be mindful of language and use of profanity to encourage a family-friendly environment.\\n\\nShift working hours for a cause\\nCaregiving, outreach programs, and community service do not conveniently wait for regular business hours to conclude. If there‚Äôs a cause or community effort taking place, feel welcome to work with your manager and shift your working hours to be available during a period where you‚Äôll have the greatest impact for good. For colleagues supporting others during these causes, document everything and strive to post recordings so it‚Äôs easy for them to catch up.\\n\\nBe a mentor\\nPeople feel more included when they‚Äôre supported. To encourage this, and to support diversified learning across departments, consider GitLab‚Äôs Internship for Learning program.\\n\\nCulture fit is a bad excuse\\nWe don‚Äôt hire based on culture or select candidates because we‚Äôd like to have a drink with them. We hire and reward team members based on our shared values as detailed on this page. We want a values fit, not a culture fit. We want cultural diversity instead of cultural conformity. Said differently: ‚Äúculture add‚Äù > ‚Äúculture fit‚Äù or ‚Äúhire for culture contribution‚Äù since our mission is that everyone can contribute.\\n\\nReligion and politics at work\\nWe generally avoid discussing politics or religion in public forums because it is easy to alienate people that have a minority opinion. This doesn‚Äôt mean we never discuss these topics. Because we value diversity, inclusion and belonging, and want all team members to feel welcome and contribute equally, we encourage free discussion of operational decisions that can move us toward being a more inclusive company. GitLab also publicly supports pro diversity, inclusion & belonging activities and events.\\n\\nThere is sometimes a grey area where advocating for diversity and political activities may intersect. Team members should use discretion in grey area communications, because a culture of belonging requires us to be respectful of the broad spectrum of views within our work environment. What does this mean in practice? Please feel empowered to share information that highlights diversity, inclusion and belonging issues and how GitLab and GitLab team members can get involved. In line with our Code of Business Conduct and Ethics, avoid posting articles that reference specific political figures or parties.\\n\\nWhile it is acceptable for individuals to bring up politics and religion in social contexts such as coffee chats and real-life meetups with other coworkers (with the goal to understand and not judge), always be aware of potential sensitivities, exercise your best judgment, and make sure you stay within the boundaries of our Code of Business Conduct and Ethics.\\n\\nWe‚Äôre a global company where perspectives and local norms may differ from culture to culture. Diversity, inclusion and belonging is about broad inclusion at a worldwide level. If there is a question or concern, please reach out to diversityinclusion@gitlab.com or #diversity_inclusion_and_belonging.\\n\\nQuirkiness\\nUnexpected and unconventional things make life more interesting. Celebrate and encourage quirky gifts, habits, behavior, and points of view. Open source is a great way to interact with interesting people. We try to hire people who think work is a great way to express themselves.\\n\\nBuilding a safe community\\nDo not make jokes or unfriendly remarks about characteristics of the people who make up GitLab and how they identify. Everyone has the right to feel safe when working for GitLab and/or as a part of the GitLab community. We do not tolerate abuse, harassment, exclusion, discrimination, or retaliation by/of any community members, including our team members. You can always refuse to deal with people who treat you badly and get out of situations that make you feel uncomfortable.\\n\\nUnconscious bias\\nWe recognize that unconscious bias is something that affects everyone and that the effect it has on us as humans and our company is large. We are responsible for understanding our own implicit biases and helping others understand theirs. We are continuously working on getting better at this topic.\\n\\nInclusive benefits\\nWe list our Parental Leave publicly so people don‚Äôt have to ask during interviews.\\n\\nInclusive language & pronouns\\nUse inclusive language. For example, prefer ‚ÄúHi everybody‚Äù or ‚ÄúHi people‚Äù to ‚ÄúHi guys‚Äù, and ‚Äúthey‚Äù instead of ‚Äúhe/she‚Äù. While there are several good guides from folks like 18f, University of Calgary, and Buffer on using inclusive language, we don‚Äôt keep an exhaustive list. When new possibly non-inclusive words arise, we prefer to be proactive and look for an alternative. If your goal is to be inclusive, it is more effective to make a small adjustment in the vocabulary when some people have a problem with it, rather than making a decision to not change it because some people don‚Äôt think it is a problem. And if you make a mistake (e.g. accidentally using the wrong pronoun or an outdated phrase), acknowledge it, apologize gracefully and move on; there is no need to dwell on it, and you can work to avoid making that mistake in the future. Please also visit our Gender and Sexual-orientation Identity Definitions and FAQ page if you have questions around pronouns and other topics related to gender / sexual orientation.\\n\\nLearn how to pronounce other people‚Äôs names\\nWe attach part of our identity to our names, and if it is mispronounced it can feel less inclusive. If it happens repeatedly, you may be unintentionally sending a message to that person that you are not interested in learning how to pronounce their name correctly. This applies to everyone you are in contact with: team members, customers, candidates for jobs, and anyone else.\\n\\nPeople whose name is repeatedly mispronounced might feel unimportant or self-conscious, and might not speak up about it. Other negative behaviors include giving a person a nickname without their permission, or actively avoiding using their name in sync calls.\\n\\nIt might be challenging to pronounce names from a different language or culture than your own, but with some effort, name pronunciation can be learned by anyone. Some ways to achieve this are:\\n\\nAsk the person for help in a private space: ‚ÄúI‚Äôm sorry, I don‚Äôt think I am pronouncing your name correctly. Can you help me with the correct pronunciation?‚Äù\\nUse the written and recorded pronunciation tools in Slack.\\nUse online tools such as videos recorded on YouTube or NameShouts.\\nPractice the pronunciation with a friend or team member who knows the correct pronunciation.\\nAlways avoid making jokes or comments about how it is difficult to pronounce their name.\\nUse of nicknames\\nSome people might choose to use a nickname, for example: ‚ÄúBob‚Äù instead of ‚ÄúRobert‚Äù. As long as this is their choice this is perfectly acceptable. We should avoid assigning a nickname to a person without their permission.\\n\\nSlack pronunciation features\\nSlack has two features to help with this issue: the phonetic name pronunciation field and the ability to record your own name pronunciation audio clip. We encourage all team members to complete both of these. Update them by editing your profile.\\n\\nInclusive interviewing\\nThis is documented on our page about interviewing.\\n\\nInclusive meetings\\nBe consciously inclusive in meetings by giving everyone present an opportunity to talk and present their points of view. This can be especially important in a remote setting.\\n\\nWith internal meetings, consider using an agenda document for questions. For example, with GitLab Group Conversations, every meeting has a numbered list that GitLab team members can add questions to. During the meeting, questions are answered in turn and discussions noted in the same document. Sometimes, these documents can have so much traffic (during the meeting) such that only a limited number of people can edit the document. In these situations, those who have questions should post on zoom chat and those who can edit the document should help copy the question over to the document. In addition, those who can edit the document should also post in zoom chat to see if anyone has any questions that they could help add to the document so that meeting attendees are more empowered to contribute to the conversation.\\n\\nCustomers are not used to working in this way. To promote inclusion with customers: ask participants for their goals; make sure during demos that you pause for question; leave time for discussion.\\n\\nInclusive and fair policy to regions with fewer employees\\nBeing globally distributed has the benefit that someone can cover for you when you are off work. However, population density is not balanced across timezones. Policies should remain fair to those in less dense regions.\\n\\nFor example, the Asia Pacific region covers more timezones but has fewer team members. If we use an algorithm to assign tasks to those in later timezones, all American tasks would fall on the fewer Asia Pacific employees. This can damage belonging and inclusivity and should be avoided.\\n\\nWhen planning an event, the organizer should cater for location density differences to maximize participation in all regions.\\n\\nSee Something, Say Something\\nAs a globally-dispersed company, we have team members from many different backgrounds and cultures. That means it is important for each of us to use great judgment in being respectful and inclusive of our teammates. At the same time, we may sometimes not fully realize we have said or done something to offend someone. It is important that our teammates hold each other accountable and let them know if they have unintentionally or intentionally done something so they can learn and gain additional understanding of perspectives different from our own. It is also important that our teammates don‚Äôt feel excluded or minimized by the words we use or the things we do. Thus, we all need to speak up when we see something that isn‚Äôt respectful or inclusive.\\n\\nEmbracing Neurodiversity\\nNeurodiversity refers to variations in the human brain regarding learning, attention, sociability, mood, and other mental functions. There are various neurodevelopmental conditions, like autism, ADHD, dyslexia, dyscalculia, dyspraxia, cognitive impairment, schizophrenia, bipolarity, and other styles of neurodivergent functioning. While neurodivergent individuals often bring unique skills and abilities which can be harnessed for a competitive advantage in many fields (for example, cybersecurity), neurodivergent individuals are often discriminated against. Due to non-inclusive hiring practices, they sometimes have trouble making it through traditional hiring processes. Neurodiversity inclusion best practices benefit everyone, and at GitLab, everyone can contribute. The handbook, values, strategy, and interviewing processes must support the ability for everyone to thrive.\\n\\nAt GitLab we embrace Neurodiversity through adopting a variety of different work styles and communication styles, and we lean into transparency, asynchronous as a default working style, and pre-filled meeting agendas. These best practices become even more important when embracing neurodiversity. Providing multiple ways to consume information (written / video / audio) allows everyone to contribute independent of their preferred comprehension style. It is important to ask team members specifically what their preferred communication method is in order to provide them information in a format that is easily consumable for them.\\n\\nRemember, brains work differently and always assume positive intent, even if someone behaves in an unexpected way. While it may be an unexpected behavior to you, it may not be unexpected to the individual exhibiting the behavior. That is the beauty and value of diversity, embracing differences and becoming stronger and better as a result.\\n\\nWe also recommend that all team members review the Reasonable Accommodation process. A Reasonable Accommodation for a team member could include noise-cancelling headphones, scheduling smaller group session zoom calls, providing very explicit and precise instructions and due-dates when given tasks, or providing a variety of supportive software tools.\\n\\nThe most important thing that managers can do is facilitate an environment in which all team members feel psychologically safe enough to make requests for what they need in order to do their job.\\n\\nFamily and friends first, work second\\nLong-lasting relationships are the rocks of life, and come before work. As someone said in our #thanks channel after helping a family member for five days after a hurricane: ‚ÄúTHANK YOU to GitLab for providing a culture where ‚Äúfamily first‚Äù is truly meant‚Äù. Use the hashtag: #FamilyAndFriends1st\\n\\nEquity not just equality\\nEquity vs. Equality: What‚Äôs the Difference?\\n\\nWhile the terms equity and equality may sound similar, the implementation of one versus the other can lead to dramatically different outcomes for marginalized people.\\n\\nEquality means each individual or group of people is given the same resources or opportunities. Equity recognizes that each person has different circumstances and allocates the exact resources and opportunities needed to reach an equal outcome.\\n\\nDiversity, Inclusion & Belonging Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate diversity, inclusion and belongings when we foster an environment where everyone can thrive and ensuring that GitLab is a place where people from every background and circumstance feel like they belong and can contribute.\\n\\nIf you would like to improve your skills or expand your knowledge on topics relating to Diversity, Inclusion, & Belonging at GitLab, check out our resources:\\n\\nBeing an Ally\\nBeing Inclusive\\nRecognizing Bias\\nGitLab Job Grade\\tDemonstrates Diversity & Inclusion Competency by‚Ä¶\\tDemonstrates DIB Behaviors by‚Ä¶\\n(Should not be considered an exhaustive list)\\tKnowledge Assessment\\n4\\tLearns to understand the impact of biases. Gathering more information about the skills needed to be accountable for their actions, apologizes and learn.\\n5\\tDevelops an understanding of the impact of biases; seeks to learn more about their own biases. Is accountable for their actions, apologizes and learns from their mistakes.\\nDIB training and/or other company wide training to further education on DIB\\nAttend DIB Initiatives Calls to stay informed and connected with ongoing efforts and discussions\\nParticipate in a DIB Initiative: join TMRGs you identify with, support other groups as an Ally, attend events, \"like\" slack posts and spread the word\\nParticipate in a TMRG initiative\\nMember of a working group related to a DIB initiative, e.g. participate in a Mentorship program\\nKnowledge Assessment for Individual Contributors\\n6\\tHas a growing understanding of the impact of biases; fosters a sense of inclusion and belonging on their team. Holds themselves and peers accountable for upholding this value by kindly pointing out when mistakes might be made. Encourages an inclusive team environment where differences are encouraged and everyone can contribute.\\n7\\tActively aware of how bias or exclusion might occur on a team and helps to facilitate a team environment where team members belong and feel safe. Models empathy with their interactions with customers and cross functional team members.\\n8\\tImplements best practices to limit bias on their team. They ensure blameless accountability is practiced throughout their team. Creates an environment where team members feel safe to share ideas and welcomes individual differences.\\nDIB training and/or other company wide training to further education on DIB\\nAttend DIB Initiatives Calls to stay informed and connected with ongoing efforts and discussions\\nParticipate in a DIB Initiative: join TMRGs you identify with, support other groups as an Ally, attend events, like posts and spread the word\\nParticipate in a TMRG initiative\\nMember of a working group related to a DIB initiative, e.g. participate in a Mentorship program\\nHiring Manager ensure a diverse candidate slate and interview panel\\nActive participant and advocate for department DIB goals\\nKnowledge Assessment for People Leaders\\n9\\tProactively finds ways of facilitating an inclusive team environment and assesses processes to protect against unconscious bias. They hold their team members accountable including cross functional stakeholders. Promotes individual differences across their team and other departments.\\n10\\tDrives diversity, inclusion and sense of belonging across their department. They hold their managers and peers accountable for upholding this value. They are actively involved in the execution of D&I strategies and encourage others to participate.\\nBlack is Tech, Grace Hopper\\n11\\tEmbeds the value of Diversity & Inclusion across their division and finds opportunities to limit the impact of bias on decision making processes. Uses feedback and data to formulate a strategy on how to make improvements. They hold their management teams accountable for upholding the value.\\n12\\tLeads with the value of Diversity & Inclusion across the company and finds opportunities to limit the impact of bias on decision making processes. They sponsor internal initiatives to increase trust, psychological safety and inclusion. They hold their senior management and the e group accountable for upholding this value.\\nDIB Team & Leadership DIB Council to establish an action plan for your departments & division\\nServe as an TMRG executive sponsor\\nEmbed DIB into All hands or in person events e.g.review organizations OKR as it relates to DIB progress, champion trainings related to further DIB knowledge, invite guest speakers to advocate\\nEVP/CXO\\tChampions the value of Diversity, Inclusion and Belonging into the company\\'s strategy. They champion and sponsor internal and external D&I initiatives. They speak to the importance of this value in company-wide meetings. They hold their leaders and other e group members accountable for upholding this value. They continuously seek ways to increase trust, psychological safety and inclusion across the broader company.\\nYearlies initiative focused on continuing to build a diverse team of top talent that we retain and grow\\nAdvocate and Integrate DIB into your organizational KPIs/OKRs, e.g.OKR: continue to build and grow a diverse workforce/organization\\nPartner with the DIB Team & Leadership DIB Council to establish an action plan for your departments & division\\nServe as a TMRG executive sponsor\\nEmbed DIB into All hands or in person events e.g.review organizations OKR as it relates to DIB progress, champion trainings related to further DIB knowledge, invite guest speakers to advocate\\nüë£ Iteration\\nMerriam-Webster defines iteration as the ‚Äúthe action or a process of iterating or repeating: such as a procedure in which repetition of a sequence of operations yields results successively closer to a desired result.‚Äù At GitLab, we iterate to do the smallest viable and valuable thing, and get it out quickly for feedback. This thing may be additive (adding something) or subtractive (removing something). If you make suggestions that can be excluded from the first iteration, turn them into a separate issue that you link. While you should have a clear vision of the desired outcome, don‚Äôt write a large plan; only write the first step. Trust that you‚Äôll know better how to proceed after something is released. You‚Äôre doing it right if you‚Äôre slightly embarrassed by the minimal feature set shipped in the first iteration. This value is the one people most underestimate when they join GitLab. The impact both on your work process and on how much you achieve is greater than anticipated. In the beginning, it hurts to make decisions fast and to see that things are changed with less consultation. But frequently, the simplest version turns out to be the best one.\\n\\nPeople that join GitLab all say they already practice iteration. But this is the value that is the hardest to understand and adopt. People are trained that if you don‚Äôt deliver a perfect or polished thing, there will be a problem. If you do just one piece of something, you have to come back to it. Doing the whole thing seems more efficient, even though it isn‚Äôt. If the complete picture is not clear, your work might not be perceived as you want it to be perceived. It seems better to make a comprehensive product. They see other GitLab team members being really effective with iteration but don‚Äôt know how to make the transition, and it‚Äôs hard to shake the fear that constant iteration can lead to shipping lower-quality work or a worse product. It is possible to ship a minimally viable product while continuing to adhere to the documented quality standards.\\n\\nThe way to resolve this is to write down only what value you can add with the time you have for this project right now. That might be 5 minutes or 2 hours. Think of what you can complete in that time that would improve the current situation. Iteration can be uncomfortable, even painful. If you‚Äôre doing iteration correctly, it should be. Reverting work back to a previous state is positive, not negative. We‚Äôre quickly getting feedback and learning from it. Making a small change prevented a bigger revert and made it easier to revert.\\n\\nHowever, if we take smaller steps and ship smaller, simpler features, we get feedback sooner. Instead of spending time working on the wrong feature or going in the wrong direction, we can ship the smallest product, receive fast feedback, and course correct. People might ask why something was not perfect. In that case, mention that it was an iteration, you spent only ‚Äúx‚Äù amount of time on it, and that the next iteration will contain ‚Äúy‚Äù and be ready on ‚Äúz‚Äù.\\n\\nIteration enables results and efficiency\\n\\n\\nIn the GitLab Unfiltered video embedded above, GitLab CEO and co-founder Sid Sijbrandij shares key operating principles to reinforce iteration in an organization.\\n\\nStart with a long-term vision\\nIteration involves driving results in pursuit of a long-term vision. While the intermediate goals may change as we iterate, we are unlikely to be successful if we don‚Äôt start with a vision of what we are working toward. Shipping that vision in iterations allows us to learn from customers using it and adjust the vision if needed. Iteration for the sake of iteration can lead to inefficiencies and not deliver desired results.\\n\\nDon‚Äôt wait\\nDon‚Äôt wait. When you have something of value like a potential blog post or a small fix, implement it straight away. Right now, everything is fresh in your head and you have the motivation. Inspiration is perishable. Don‚Äôt wait until you have a better version. Don‚Äôt wait until you record a better video. Don‚Äôt wait for an event (like Contribute). Inventory that isn‚Äôt released is a liability since it has to be managed, becomes outdated, and you miss out on the feedback you would have received had you implemented it straight away. When we don‚Äôt wait we signal intent to others that we have a purpose to resolve something.\\n\\nIterate toward global maximum\\nIf we are not aware of interdependencies beyond our team and collaborating with others across the organization, we risk deliverables that settle into a ‚Äúlocal maxima‚Äù of quality, richness, and efficiencies. This localization is largely defined by team structure and organizational boundaries. While an iteration can take place within a single team, that team is responsible for identifying inter-dependencies and proactively communicating and aligning with other teams working on related projects. This helps ensure that iterations are not ‚Äúhalf-baked‚Äù and align with work being done across the broader organization.\\n\\nSet a due date\\nWe always try to set a due date. If needed, we cut scope. If we have something planned for a specific date, we make that date. For example we shipped over 133 monthly releases. But every one of them doesn‚Äôt contain all the features we planned. If we planned an announcement for a certain date, we might announce less or indicate what is still uncertain. But we set a due date because having something out there builds trust and gives us better feedback.\\n\\nCleanup over sign-off\\nAs discussed in Sid‚Äôs interview on iteration, waiting for approval can slow things down. We can prevent this with automation (e.g. tests of database migration performance) or clean-up after the fact (refactor a Pajamas if something was added that isn‚Äôt coherent), but we try to ensure that people don‚Äôt need to wait for sign-off.\\n\\nStart off by impacting the fewest users possible\\nIf you do a gradual rollout of your change prefer: few users over many users, internal users (dogfooding) over external ones, environments you get faster feedback about (SaaS) over low feedback ones (self-managed), etc.\\n\\nReduce cycle time\\nShort iterations reduce our cycle time.\\n\\nWork as part of the community\\nSmall iterations make it easier to work with the wider community. Their work looks more like our work, and our work is also quicker to receive feedback.\\n\\nMinimal Valuable Change (MVC)\\nWe encourage MVCs to be as small as possible. Always look to make the quickest change possible to improve the user‚Äôs outcome. If you validate that the change adds more value than what is there now, then do it. This may be additive (adding something) or subtractive (removing something). No need to wait for something more robust. More information is in the product handbook, but this applies to everything we do in all functions. Specifically for product MVCs, there is additional responsibility to validate with customers that we‚Äôre adding useful functionality without obvious bugs or usability issues.\\n\\nMake a proposal\\nIf you need to decide something as a team, make a concrete proposal instead of calling a meeting to get everyone‚Äôs input. Having a proposal will be a much more effective use of everyone‚Äôs time. Every meeting should be a review of a proposal. We should be brainwriting on our own instead of brainstorming out loud. State the underlying problem so that people have enough context to propose reasonable alternatives. The people that receive the proposal should not feel left out and the person making it should not feel bad if a completely different proposal is implemented. Don‚Äôt let your desire to be involved early or to see your solution implemented stand in the way of getting to the best outcome. If you don‚Äôt have a proposal, don‚Äôt let that stop you from highlighting a problem, but please state that you couldn‚Äôt think of a good solution and list any solutions you considered.\\n\\nBy making a proposal you also provide better visibility into the work and the context surrounding it.\\n\\nIn this GitLab Unfiltered video, GitLab CEO and co-founder Sid Sijbrandij converses about iteration in engineering, leveraging proposals to break work into smaller components.\\n\\nEverything is in draft\\nAt GitLab, we rarely mark any content or proposals as drafts. Everything is always in draft and subject to change. When everything is in draft, contributions from team members as well as the wider community are welcomed. By having everything in draft and assuming others have low context, confusion can be reduced as people have shared access to information.\\n\\nUnder construction\\nAs we get more users, they will ask for stability, especially in our UX. We should always optimize for the long term. This means that users will be inconvenienced in the short term, but current and future users will enjoy a better product in the end.\\n\\nEducating users on the longer-term plan helps create a shared understanding of how a small change will incrementally grow into something more. For example, we could share how a dropdown will evolve into a much more nuanced solution in the future. We can take the following steps to articulate our plan:\\n\\nOpen a feedback issue that provides context about the initial MVC (example)\\nEnsure the direction page articulates a long-term plan (example)\\nAnnounce the MVC in a release post, link to the feedback issue, and link to the direction page (example)\\nLow level of shame\\nWhen we talked to Nat Friedman, he said: ‚ÄúA low level of shame is intrinsic to your culture.‚Äù This captures the pain we feel by shipping something that isn‚Äôt where we want it to be yet.\\n\\nGitLab Head of Remote Darren M. adds context on this operating principle.\\n\\nIn many organizations, you take a risk when you put forth any work that‚Äôs not perfect ‚Äî where you haven‚Äôt spent endless cycles planning for contingencies or counterpoints. Because of this, you‚Äôre incentivized to invest a lot of time and effort into preparing for ‚ÄòWhat if?‚Äô scenarios before any work is presented.\\n\\nThe downside to that is clear. If you do eventually put forth the work, but it needed to be course corrected a long time ago, you‚Äôve now squandered a lot of time that you could have spent improving it via iteration.\\n\\nHaving a low level of shame requires you to combat a natural inclination to conceal work until it‚Äôs perfect, and instead celebrate the small changes.\\n\\nCultural lens\\nCultural differences can bring unique challenges and expectations to iteration. For some, expressions like ‚Äúit doesn‚Äôt have to be perfect‚Ä¶‚Äù can challenge cultural norms. We encourage you to bring your authentic self and seek shared understanding when iterating. Giving feedback and ensuring psychological safety are necessary for every iterative attempt.\\n\\nFocus on improvement\\nWe believe great companies sound negative because they focus on what they can improve, not only on what is working well. In every conversation, inside and outside the company, we should ask a question: What do you think we can improve? This doesn‚Äôt mean we don‚Äôt recognize our successes; for example, see our Say Thanks value.\\n\\nWe are positive about the future of the company. We are Short Term Critical And Long Term Optimistic (STeCALTO, for short).\\n\\nBe deliberate about scale\\nFirst, optimize for speed and results (and be deliberate about how your change affects other processes/functionality); when it is a success, figure out how to scale it. Great examples are in this article by Paul Graham.\\n\\nResist bundling\\nResist the urge to bundle a series of smaller iterations so team members don‚Äôt see a project as their last (or best) opportunity to contribute. It‚Äôs tempting to create encompassing projects or initiatives that roll many smaller projects up. This incarnation of scope creep drives up cost, encourages fewer risks, and incentivizes perfection (via longer cycle times) over progress. When we resist bundling, we reduce the risk that work will be canceled due to scale or scope. By resisting bundling we also reduce the coordination needed because fewer people or teams may be involved.\\n\\nMake two-way door decisions\\nMost decisions are easy to reverse. In these cases, the Directly Responsible Individual should go ahead and make them without approval. Only when you can‚Äôt reverse them should there be a more thorough discussion. By embracing iteration and making two-way door decisions, we are more efficient and achieve more results.\\n\\nChanging proposals isn‚Äôt iteration\\nChanging something without shipping it is a revision, not iteration. Only when the change is rolled out to users can you learn from feedback. When you‚Äôre changing a proposal based on different opinions, you‚Äôre frequently wasting time; it would be better to roll out a small change quickly and get real world feedback. Never call a revision an iteration because it is almost the opposite.\\n\\nA few challenges have arisen with how we approach iteration. The best example may be the proposal of a two-month release cycle. The argument was that a longer release cycle would buy us time for bug fixes and feature development, but we don‚Äôt believe that is the case. As detailed above, we aim to make the absolute smallest viable and valuable thing possible, and that doing otherwise will only slow us down.\\n\\nThat said, we would love to work on a two-week release cycle, but that should be another conversation.\\n\\nEmbracing Iteration\\nIn order to embrace iteration, we should have the attitude that we are trying to achieve as much as possible in a small amount of time; it‚Äôs where we are at the end state of an iteration, that counts. The benefit of iteration is to get feedback from the end-user. Focus on sharing context on the end of the first iteration rather than a hypothetical future state requiring multiple iterations. By embracing iteration we can increase creativity in incremental components.\\n\\n\\nMake small merge requests\\nWhen you are submitting a merge request for a code change, or a process change in the handbook, keep it as small as possible. If you are adding a new page to the handbook, create the new page with a small amount of initial content, get it merged quickly via Handbook Usage guidelines, and then add additional sections iteratively with subsequent merge requests. Similarly, when adding features to GitLab, consider ways to reduce the scope of the feature before creating the merge request to ensure your merge request is as small as possible.\\n\\nAlways iterate deliberately\\nRapid iteration can get in the way of results if it‚Äôs not thought out; for example, when adjusting our marketing messaging (where consistency is key), product categories (where we‚Äôve set development plans), organizational structure or product scope alignment (where real human stresses and team stability are involved), sales methodologies (where we‚Äôve trained our teams) and this values page (where we use the values to guide all GitLab team members). In those instances, we add additional review to the approval process; not to prohibit, but to be more deliberate in our iteration. The change process is documented in the GitLab Handbook Usage page and takes place via merge request approvals.\\n\\nSee it in action\\nIteration is so important to GitLab that the CEO hosted Iteration Office Hours to provide guidance and assist in breaking large, complex topics into MVCs and smaller iterations of work.\\n\\n2019-11-19\\n2020-01-15\\n2020-01-17\\n2020-02-18\\nYou can view these on our GitLab Unfiltered YouTube channel.\\n\\nExamples of iteration in other companies\\nIteration is a key value in many disruptive and successful organizations. Below are some examples:\\n\\nSpaceX\\n12 things that are not iteration\\nIteration is often counterintuitive and difficult to do. To clarify what an iteration is, it helps to see examples of what is not an iteration. Below are 12 examples of things we‚Äôve seen mistaken as iteration, but don‚Äôt meet our definition of iteration.\\n\\nReducing quality\\nAvoiding or reducing documentation\\nCompromising on security\\nDelivering something that‚Äôs not the recommended path or on by default\\nShipping something of no value\\nAn excuse to focus on unimportant items\\nChanging or lowering goal posts\\nRevisions you don‚Äôt ship or publish\\nAn excuse to impose unrealistically tight timelines\\nAn excuse to avoid planning\\nImposing long hours\\nExpecting others to fix your work\\nIn this GitLab Unfiltered video, GitLab co-founder and CEO Sid Sijbrandij elaborates on each of these 12 things that are not iteration.\\n\\nIteration Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate iteration when we do the smallest viable and valuable thing, get it out quickly for feedback, and make changes based that feedback.\\n\\nGitLab Job Grade\\tDemonstrates Iteration Competency by‚Ä¶\\tKnowledge Assessment\\n5\\tDevelops own knowledge by trying and failing. When asking questions isn\\'t content with silence or unhelpful/incomplete responses, seeks out primary sources.\\tKnowledge Assessment for Individual Contributors\\n6\\tActively looks for opportunities to iterate and contribute to boring solutions. Balances short term gains and long term benefit with team\\'s help. Ships things that aren\\'t 100% knowing that you\\'ll be able to improve them in the next revision. Asks questions with abandon. Publicly shares failures if you\\'ll help colleagues learn.\\n7\\tIndependently balances short term gains and long term benefit. Identifies opportunities to deliver projects in an iterative way.\\n8\\tIs able to take long term goals and turn them into small actionable steps that can be implemented in an iterative way. Identifies and prevents decisions that are not \"two-way door decisions\". Ships. All the time. Sounds like a broken record in discussions with more junior members of the team; always asking if we can make something smaller.\\tKnowledge Assessment for People Leaders\\n9\\tIn addition to upholding the requirements of a Staff/Manager level, a Principal/Sr. Manager practices and fosters the value of iteration to team members. They hold their team members accountable for iteration and boring solutions.\\n10\\tIn addition to upholding the requirements of a Principal/Sr. Manager, a Distinguished/Director proactively finds ways to drive the value of iteration and boring solutions.\\n11\\tIn addition to upholding the requirements of a Distinguished/Director , a Sr. Distinguished/Sr. Director embeds the value of Iteration across the department and division. They use their cognitive and analytical abilities to anticipate and adapt to unpredictabilities in regard to strategic risk in a way that benefits all involved.\\n12\\tIn addition to upholding the requirements of a Sr. Distinguished/Sr. Director , a Fellow/VP leads the way for the value of Iteration across the division and cross functional teams. They confidently lead their teams through change and proactively take risks based on values and the strategic vision.\\nEVP/CXO\\tIn addition to upholding the requirements of a Fellow/VP, the EVP champions the value of Iteration across GitLab. They are comfortable leading through discomfort and the unease associated with change and innovation.\\nüëÅÔ∏è Transparency\\nBe open about as many things as possible. By making information public, we can reduce the threshold to contribution and make collaboration easier. Use public issue trackers, projects, and repositories when possible. Transparency is not communication. Just because something exists in the handbook or elsewhere doesn‚Äôt mean it‚Äôs been communicated to the people who should understand or acknowledge it. On a personal level, be direct when sharing information, and admit when you‚Äôve made a mistake or were wrong. When something goes wrong, it is a great opportunity to say ‚ÄúWhat‚Äôs the kaizen moment here?‚Äù and find a better way without hurt feelings.\\n\\nEven as a public company, we know that our value of transparency will be key to our success. This value can be hard to follow at times. You might ask yourself: what should be shared, how much to share, whether or not to speak up but definitely take the time to always opt for maximum transparency by adhering to the operating principles below. Often, company values get diluted as they grow, most likely because they do not write anything down. But we will make sure our values scale with the company. As a public company, we declare everyone in the company as an insider, which allows us to remain transparent internally about our numbers, etc. Everything else that can be transparent will continue to be so.\\n\\nWhen there are exceptions, material that is not public by default is documented.\\n\\nPublic by default\\nEverything at GitLab is public by default. The public process does two things: allows others to benefit from the conversation and acts as a filter. Since there is only a limited amount of time, we prioritize conversations that a wider audience can benefit from.\\n\\nOne example of transparency at GitLab is the public repository of this website that also contains this company handbook. Others include the GitLab CE and GitLab EE issue trackers, as well as marketing and infrastructure. Transparency creates awareness for GitLab, allows us to recruit people that care about our values, gets us more and faster feedback from people outside the company, and makes it easier to collaborate with them. It is also about sharing great software, documentation, examples, lessons, and processes with the whole community and the world in the spirit of open source, which we believe creates more value than it captures.\\n\\nIn line with our value of transparency and being public by default, all GitLab team member profiles should be public. Public profiles also enable broader collaboration and efficiencies between teams. To do so, please make sure that the checkbox under the Private profile option is unchecked in your profile settings. If you do not feel comfortable with your full name or location on your profile, please change it to what feels appropriate to you as these are displayed even on private profiles.\\n\\nBecause we are public by default and have the SAFE framework we don‚Äôt need to make cases for why things should be transparent. If something is unSAFE and needs to remain not public it can be.\\n\\nNot public\\nWe make information public by default because transparency is one of our values. However it is most important to focus on results. Therefore, a category of information is public unless there is a reason for it not to be. If something is not public, there should be a reference in the handbook that states a confidential decision was taken with a link to our Not Public guidelines, unless legal feels it carries undue risk. We document what is not public by default on our communication page.\\n\\nIf you believe something shouldn‚Äôt be public that currently is (or vice versa), then make a merge request to the relevant page(s) suggesting the change so that you can collaborate with others and discuss with the DRI. When content contains information which is not public it is recommended to remove the specific sections which are not public, put them on their own page in the internal handbook, and then link out to that with a ‚Äúnot public/internal only‚Äù note. Always share publicly what we can.\\n\\nWhen information is not public, it may also be treated as limited access, only shared with certain GitLab roles, teams, or team members due to privacy considerations, contractual obligation, or other reasons that the author or DRI can specify. Certain kinds of information default to limited access, including details about team members or customers who did not give permission to share the information.\\n\\nMost companies become non-transparent over time because they don‚Äôt accept any mistakes. Instead, we should always err on the side of transparency when there is a choice to be made between caution or inaction, and transparency. If we make a mistake, we now know what the limits of transparency are for the company and we should document this. The only exception to this rule would be in the case when there are legal concerns.\\n\\nBecause some information is not public the public information can be lacking some context. We should be cognizant of that.\\n\\nDirectness\\nBeing direct is about being transparent with each other. We try to channel our inner Ben Horowitz by being both straightforward and kind. Feedback is always about your work and not your person. That doesn‚Äôt mean it will be easy to give or receive it.\\n\\nArticulate when you change your mind\\nIf you state one thing, and then change course and support a different direction, point, or outcome, articulate this. It is OK to have your position changed by new data. Articulating that an earlier stance is not your current stance provides clarity to others and encourages data-driven decision making.\\n\\nSurface issues constructively\\nBe transparent to the right people (up) at the right time (when still actionable). If you make a mistake, don‚Äôt worry; correct it and proactively let the affected party, your team, and the CEO know what happened, how you corrected it, and how‚Äîif needed‚Äîyou changed the process to prevent future mistakes.\\n\\nTransparency is most valuable if you continue to do it when there are costs\\nWe practice transparency even when hiding the facts would be easier. For example, many companies do not give you the real reason why they declined your application because it increases the chance of legal action. We want to only reject people for the right reasons and we want to give them the opportunity to grow by getting this feedback. Therefore, we‚Äôll accept the increased risk of holding ourselves to a high standard of making decisions and do the right thing by telling them what we thought. Other examples are being transparent about security incidents and participating in and contributing to Live Broadcasts.\\n\\nTransparency has costs (distraction, mis-interpretation, etc.) but also great benefits (productivity, hiring, retention, brand awareness, etc). Team members can view more details on these benefits by referencing the ‚ÄúTransparency Benefit Quantification‚Äù slides in Google Drive). We should carefully weigh the tradeoff between costs and benefits, to prevent a knee-jerk reaction to reduce transparency when it has costs.\\n\\nSingle Source of Truth\\nBy having most company communications and work artifacts be public to the Internet, we have one single source of truth for all GitLab team members, users, customers, and other community members. We don‚Äôt need separate artifacts with different permissions for different people.\\n\\nFindability\\nOur transparency value means more than just making information accessible to all. In order to improve performance it‚Äôs important that we not only ensure information is accessible, but also ensure it flows to the correct places and is findable by those who need it. Focusing on information flow will ensure you, for example, utilize multi-modal communication, or that you keep your stakeholders informed of changes by posting links to MRs in Slack.\\n\\nSay why, not just what\\nTransparent changes have the reasons for the change laid out clearly along with the change itself. This leads to fewer questions later on because people already have some understanding. A change with no public explanation can lead to a lot of extra rounds of questioning, which is less efficient.\\n\\nThis also helps with institutional memory: a year from now when you want to know why a decision was made, or not, the issue or MR that has the decision also shares why the decision was made. This is related to Chesterton‚Äôs fence - it‚Äôs much easier to suggest removing or changing something if you know why it exists in the first place.\\n\\nAvoid using terms such as ‚Äúindustry standard‚Äù or ‚Äúbest practices‚Äù as they are vague, opaque, and don‚Äôt provide enough context as a reason for a change.\\n\\nSimilarly, merely stating a single value isn‚Äôt a great explanation for why we are making a particular decision. Many things could be considered ‚Äúiteration‚Äù or ‚Äúefficiency‚Äù that don‚Äôt match our definition of those values. Try to link to an operating principle of the value or provide more context, instead of just saying a single value‚Äôs name.\\n\\nSaying why and not just what enables discussion around topics that may impact more than one value; for instance, when weighing the efficiency of boring solutions with the focus on customer results. When decisions align with all of our values, they are easy to discuss and decide. When there are multiple values involved, using our values hierarchy and directly discussing the tradeoffs is easier with more context.\\n\\nArticulating why also helps people understand how something changed when you articulate that you changed your mind.\\n\\nSaying why does not mean justifying a decision against all other suggestions. The DRI is responsible for their decision. The DRI is not responsible for convincing other people, but they should be able to articulate their reasoning for the change.\\n\\nWhen a GitLab Team Member comes across an ask or material (MR, handbook, etc.) that does not provide a ‚Äúwhy‚Äù with sufficient context, the Team Member is responsible for getting the why and, if needed, working with the DRI to ensure that it is adequately documented and communicated to give context to other team members. In the absence of a why, team members may speculate the why. This is something that can lead to disruption and inefficiency.\\n\\nReproducibility\\nEnable everybody involved to come to the same conclusion as you. This not only involves reasoning, but also providing, for example: raw data and not just plots; scripts to automate tasks and not just the work they have done; and documenting steps while analyzing a problem. Do your best to make the line of thinking transparent to others, even if they may disagree.\\n\\nTransparency Competency\\nCompetencies are the Single Source of Truth (SSoT) framework for things we need team members to learn. We demonstrate transparency when we are open with as many things as possible reducing the threshold to contribution and make collaboration easier.\\n\\nGitLab Job Grade\\tDemonstrates Transparency Competency by‚Ä¶\\tKnowledge Assessment\\n5\\tUses public issue trackers, projects, and repositories when possible. Looks for opportunities to publicly share the things that they are working on.\\tKnowledge Assessment for Individual Contributors\\n6\\tProvides context and background on projects and issues so that those with no prior knowledge are able to contribute to the discussion. They welcome feedback and new ideas as they know that will lead to a better solution.\\n7\\tContinually surfaces improvements across their functional area of expertise. They share feedback with others and understand how to disagree and commit to solutions. They model what it means to be as open as possible. They encourage conversation in public channels.\\n8\\tImplements open processes across their team. They also track team issues and projects openly so their team members are aware of everything that is happening on a team at a given time. They leverage feedback to drive the best possible outcomes with the information they have available. They also share feedback with their team and their peers in a timely, kind manner so their position on a given topic is known.\\tKnowledge Assessment for People Leaders\\n9\\tFosters and coaches openness across cross functional departments. They lead cross functional issues, projects and ideas inviting feedback to generate the best possible solution. They hold their teams accountable to continue to find opportunities to share things openly. They give feedback to their team members, peers and managers in a timely, kind manner so their position on a topic is known.\\n10\\tDrives their departmental strategy with openness as a key value. They hold their management team accountable to working openly and pushes them to make everything transparent even when it might be difficult to do so. They coach managers on the value that additional feedback can bring to the end solution.\\n11\\tDevelops leaders that work openly and continue to provide timely, kind feedback across their division. They develop leaders that drive their teams with openness as a foundational part of the way that they operate.\\n12\\tLeads the company by being open in all things. They are open with things that might traditionally not be shared broadly. They communicate directly and provide feedback in a timely manner to initiatives happening within their department and across the company. They hold the e group and other leaders accountable for upholding this value.\\nEVP/CXO\\tChampions transparency both internally, across the company and externally. They participate both internally and externally in events and share the value that being open can provide to increasing trust with team members and others that interact with our product. They provide timely, kind feedback with initiatives happening internally and externally. They hold the e group and other leaders accountable for upholding this value.\\nWhy have values\\nOur values provide guidelines on how to behave and are written to be actionable. They help us describe the type of behavior that we expect from GitLab team members. They help us to know how to behave in the organization and what to expect from others.\\n\\nValues provide a framework for distributed decision making, detailed in GitLab‚Äôs TeamOps management philosophy. They allow individuals to determine what to do without asking their manager and they allow teams to make consistent decisions. When teams across the organization reference the same values in their decision making, there is consistency in how decisions are made. This ensures that our culture remains driven by our values.\\n\\nLastly, values create a conscious culture that is designed to help you prosper and experience exceptional personal growth through work.\\n\\nFive dysfunctions\\nOur values also help us to prevent the five dysfunctions:\\n\\nFear of conflict Seeking artificial harmony over constructive passionate debate => prevented by transparency, specifically directness and collaboration, specifically short toes\\nAbsence of trust Unwilling to be vulnerable within the group => prevented by collaboration, specifically kindness\\nAvoidance of accountability Ducking the responsibility to call peers on counterproductive behavior which sets low standards => prevented by results, iteration, and transparency\\nInattention to results Focusing on personal success, status, and ego before team success => prevented by results\\nLack of commitment Feigning buy-in for group decisions creates ambiguity throughout the organization => prevented by transparency, specifically directness\\nSome dysfunctions are not addressed directly by our values; for example, trust is not one of our values. Similar to happiness, trust is something that is an outcome, not something you can strive for directly. We hope that the way we work and our values will instill trust, instead of mandating it from people; trust is earned, not given.\\n\\nOperating principles\\nOperating principles are behaviors that empower GitLab team members to definitively live out a given value. They clarify what a given core value means and looks like at GitLab. Understanding this distinction is critical to thriving at GitLab, particularly for newer team members who may be familiar with a prior organization‚Äôs interpretation of iteration or collaboration (as examples).\\n\\nProcess for removing operating principles\\nValues are not just things we do, but things that actively drive good behavior. When we remove them it doesn‚Äôt mean we stopped believing in it, just that it wasn‚Äôt actively helping to drive behavior. If we don‚Äôt prune our operating principles, then we will be like every other company: things that make sense but are not leading to a better culture.\\n\\nTo remove an operating principle from the Handbook page, submit your change through a merge request and explain your reasons in the merge request description.\\nThe GitLab Value Handbook Page owner must approve and merge the request.\\nMention the specific value\\nMost companies have a list of values. In companies without strong values, folks often use generalizations when they refer to values. For example, ‚Äúnot a value add‚Äù or ‚Äúscored well on values during our interview.‚Äù In companies with strong values, folks name the specific, relevant value as it applies to a given topic or situation. Values are only powerful when they are individually understood and applied by team members.\\n\\nHow to scale the business while preserving GitLab values?\\nFor certain business decisions or projects (such as compensation and end-point management ), GitLab team members may have a lot of opinions and interest, and they want to provide their feedback and comments. On the other hand, it might be challenging for the project DRI to digest and respond to all these inputs. What should you do in this scenario?\\n\\nEveryone can contribute at GitLab. We encourage team members to share feedback and leave comments on issues. Leaving feedback and comments shows that team members care about a topic and about GitLab as a company. These perspectives may also uncover potential risks and problems in the project.\\n\\nThere shouldn‚Äôt be a ‚ÄúDon‚Äôt they have their job to do?‚Äù type of response. Furthermore, we shouldn‚Äôt judge team members who are perceived as being the ‚Äúsqueaky wheel.‚Äù At GitLab, we measure results, not hours. As long as a team member is producing required results, they are empowered to decide how to spend their time.\\n\\nOn the other hand, as GitLab grows in size, we need to make decisions and the decisions may not be agreed to by everyone. If a decision or project is sensitive or controversial, and receives large amounts of feedback, it can be challenging for the project DRI to handle. In these cases, it‚Äôs best to have time-boxed feedback built into timelines.\\n\\nIn a hypothetical example where a DRI needs to decide between red and gold potatoes for a stew, they would create an issue with the following sentiment:\\n\\nWe‚Äôre deciding between red potatoes and gold potatoes to go into the stew. We have to decide by Tuesday 2020-07-14 so that we can get our order to the grocery store on Wednesday 2020-07-15. We‚Äôll be collecting input and feedback until that point. Jane is the DRI and will make the decision on 2020-07-14 with all the information we have at that point. Here is the framework we‚Äôre using for the decision:\\n\\nare there allergies to consider?\\ncost per pound\\nteam member preferences\\nOnce the decision is made, it will be what is going into the stew.\\n\\nThis method has shown itself to be effective at soliciting productive feedback that doesn‚Äôt derail a timeline while ensuring team members feel heard.\\n\\nWhy our values are public\\nCompanies are encouraged to copy and implement GitLab‚Äôs values. They are Creative Commons and can be copied verbatim.\\n\\nWe make our values public for the same reasons we make our OKRs (Objectives and Key Results) and strategy public. There is great power and efficiency in teams who share company values. Concealing values until after someone is hired into an organization is not a wise strategy.\\n\\nNot everyone will see our values and feel aligned with them, and that‚Äôs OK. By making values public, it shows respect for the time of job seekers who conduct due diligence on prospective employers. When people who are aligned with GitLab‚Äôs values apply for an open vacancy, this allows our hiring teams to more efficiently move candidates through the interview process.\\n\\nIn a GitLab Unfiltered interview on values, GitLab co-founder and CEO Sid Sijbrandij offers the following context.\\n\\nCompanies may ask you to write a blank check. They‚Äôll say, ‚ÄòCome join our organization, and when you‚Äôre here, you need to subscribe to our values, our way of working, and our strategy. It‚Äôs very essential, and it‚Äôs part of our identity!‚Äô\\n\\nBut these companies don‚Äôt give you the opportunity up front to evaluate it. It doesn‚Äôt make any sense to me. If it‚Äôs so important that people share your values, have them out there.\\n\\nHierarchy\\nOccasionally, values can contradict each other. It‚Äôs useful to keep in mind this hierarchy to resolve confusion about what to do in a specific circumstance, while remaining consistent with our core values.\\n\\nThink of the hierarchy as a weighting system. Values higher in the hierarchy do not automatically override values lower in the hierarchy. Here are some examples:\\n\\nIf a change impacts Transparency positively but impacts Efficiency negatively in roughly the same amount, we would move ahead since Transparency is higher in the hierarchy than Efficiency.\\nIf a change has a massive positive impact on Diversity but negatively impacts Iteration, we would move ahead even though Diversity is lower in the hierarchy than Iteration because the overall impact is more positive than negative.\\nValues\\n\\nIn a GitLab Unfiltered interview on values, GitLab co-founder and CEO Sid Sijbrandij offers the following context.\\n\\nIt‚Äôs an attempt to relieve at least some of the tension. It‚Äôs not absolute. If you think of values as binary, that‚Äôs not going to work. There will always be interpretation, and there‚Äôs always magnitude to consider.\\n\\nWe made a hierarchy so that it‚Äôs clear, in the end, the result matters most. For instance, we‚Äôre not going to be transparent for the sake of being transparent. We‚Äôre not radical in our transparency. We do it because we think it will lead to better outcomes.\\n\\nThose hierarchies are really important. They won‚Äôt preempt every debate, but it helps.\\n\\nUpdating our values\\nOur values are updated frequently and as needed. Everyone is welcome to make a suggestion to improve them. To update: make a merge request and assign it to the CEO. If you‚Äôre a team member or in the core team please post a link to the MR in the #values Slack channel. If you‚Äôre not part of those groups, please send a direct Twitter message to @sytses.\\n\\nHow do we reinforce our values\\nWhatever behavior you reward will become your values. We reinforce our values by:\\n\\nCriteria we use for promotions and communicate to the whole company on announcement.\\n\\nWhat we select for during hiring.\\n\\nWhat we emphasize during on-boarding.\\n\\nCriteria we use for our annual compensation review.\\n\\nWhat we refer to when making decisions.\\n\\nThe example the E-group sets for the company since a fish rots from the head down.\\n\\nWhat we expect from all team members, as ambassadors for our values.\\n\\nKeeping them up to date with a stream of commits that add details.\\n\\nBehavior we give each other 360 feedback on.\\n\\nBehavior we compliment.\\n\\nCriteria we use for discretionary bonuses.\\n\\nWhat we include in our offer letters\\n\\nCriteria we use to manage underperformance.\\n\\nWhat we do when we let people go.\\n\\nGiving value awards during Contribute.\\n\\nProviding GitLab team members and qualified individuals transparency into all aspects of the company through the CEO Shadow Program to enable them to better engage and collaborate cross-functionally.\\n\\nLinking the takeaways of courses to our values, like we did for the Crucial Conversations training.\\n\\nThe default settings of the software we use (for example: Speedy meetings, document sharing, agendas, etc.)\\n\\nReinforcing our values with features in GitLab, for example the Iterations feature.\\n\\nApplying one of our values virtual backgrounds in video calls.\\n\\nOur GitLab Song Book, the song lyrics often mention GitLab values.\\n\\nRegularly conduct a values exercise at the e-group offsite.\\n\\nThe most important moments to reinforce our values are decisions which affect individual team members most: hiring, promotions, and bonuses, which is why every promotion document at GitLab is shared with the entire company and uses the values as its core structure.\\n\\nIn negative feedback, we should be specific about what the problem is. For example, saying someone is ‚Äúnot living the values‚Äù isn‚Äôt helpful.\\n\\nYour values are what you hire for, what you praise people for, and what you promote them for. By definition, what you do in those instances are your values. It‚Äôs not what you say they are. Values should be explicitly part of our hiring process, our job profiles, and our review process.\\n\\nWhen we give bonuses and promotions, they are always linked to values. That‚Äôs the crucial thing. If you reinforce them there, that‚Äôs the most powerful thing you can do. ‚Äî Sid Sijbrandij, GitLab co-founder and CEO\\n\\nWhat to do if values aren‚Äôt being lived out\\nValue erosion can occur when indifference and apathy are tolerated. It can also occur when individuals justify undesired behaviors by interpreting values as ‚Äúme values‚Äù rather than ‚Äúcompany values.‚Äù For example, a team member may speak to the importance of personal efficiency in order to justify not collaborating professionally with peers. This is not what we expect from team members in terms of efficiency and collaboration.\\n\\nIf you feel that values are not being lived out in a given scenario, speak up and ask for context in a respectful manner. Navigating value conflicts starts with assuming positive intent from other team members. Offer links to relevant values and/or operating principles when discussing the issue. If there is confusion or disagreement about the interpretation of a value, please surface the discussion in GitLab‚Äôs #values Slack channel (for GitLab team members) or @-mentioning @gitlab on Twitter (for those who do not work at GitLab).\\n\\nIn a GitLab Unfiltered interview on values, GitLab co-founder and CEO Sid Sijbrandij offers the following context.\\n\\nAlmost every time we face a hard decision at GitLab, it‚Äôs because values are in conflict. It‚Äôs not binary logic. It requires conversation, and sometimes there is no obvious answer. We can only achieve resolution by respectfully talking with each other and trusting the DRI to make the ultimate decision.\\n\\nPermission to play\\nFrom our values we excluded some behaviors that are obvious; we call them our permission to play behavior:\\n\\nBe truthful and honest.\\nBe dependable and reliable.\\nTry to keep promises. If you might not keep a promise, proactively communicate as soon as you suspect it.\\nBe deserving of the trust of our team members, users and customers.\\nBe committed to the success of the whole organization.\\nAct in the best interest of the company, our team members, our customers, users, and investors.\\nMake the best decisions for GitLab.\\nAct in accordance with the law.\\nDon‚Äôt show favoritism as it breeds resentment, destroys employee morale, and creates disincentives for good performance. Seek out ways to be fair to everyone.\\nPlaying politics is counter to GitLab values\\nWe don‚Äôt want people to play politics at GitLab.\\n\\nAn example of politics is people discussing a proposal and being overly focused on whose proposal it is. This is a manifestation of the Belief Bias, where we judge an argument‚Äôs strength not by how strongly it supports the conclusion but by how strongly we support the conclusion. Proposals should be weighed on their merits and not on who proposed them. Another example is people being promoted based on others liking them or having a lot of alliances. We want people to be promoted based on their results. We value collaboration, but that‚Äôs different from being promoted just because people like you.\\n\\nBelow are some attributes of political and non-political work environments. GitLab plans to maintain a non-political one.\\n\\nPolitical environment\\tNon-political environment\\nValues are weaponized and used out of their intended context\\tTeam members utilize values with a positive intent\\nTeam members are driven by self-interest\\tTeam members are driven by company interest\\nTeam members work in silos\\tTeam members optimize globally\\nPeople have territorial behaviors and are quick to perceive suggestions as attacks\\tPeople have short toes\\nPeople have unhealthy alliances with backroom conversations\\tPeople have good intent and actively collaborate with folks\\nInformation is intentionally withheld\\tInformation is shared early (often WIP) and at the same time with all interested parties\\nPeople try to undermine each other‚Äôs credibility by arguing with the weakest part of their argument\\tPeople take a ‚Äústeel man‚Äù position and argue against the strongest version of your opponent‚Äôs position\\nFolks do not provide direct feedback. Instead, they withhold their thoughts or speak behind each other‚Äôs backs\\tFeedback is given directly. This includes feedback about a manager‚Äôs team\\nCommunicating your own suggestions through a report instead of directly\\tFeedback is given directly from the person who has it\\nEvaluating proposals or work by who said or did it instead of by what is in it\\tProposals and work is evaluated without regard to who worked on them\\nLack of transparency in escalations. Team members go to a manager without first attempting to align with peers on an issue or letting peers know\\tTeam members speak directly to each other about feedback and requests in order to resolve their own conflicts. When they escalate, they do it in an effective way\\n\\nValues make choices\\nValues make and clarify choices. A well-chosen value has a defensible opposite. Apple, for example, values secrecy over transparency and product perfection over iteration. They are successful building around our counter values ‚Äî although the result is a very different company.\\n\\n\\nWhat is not a value\\nAll-remote isn‚Äôt a value. It is something we do because it helps to practice our values of transparency, efficiency, results, and diversity, inclusion & belonging.\\nQuestions from new team members\\nDuring every GitLab 101 session with new hires we discuss our values. We document the questions and answers to Frequently Asked Questions about the GitLab Culture.\\n\\nNew team members should read GitLab‚Äôs guide to starting a new remote role, and reference interviews centered on values within the GitLab Unfiltered YouTube channel.\\n\\nMission\\nOur mission is that everyone can contribute. This mission guides our path, and we live our values along that path.\\n\\nMitigating Concerns\\nWe have a page which documents our Mitigating Concerns. Many of our values help to mitigate some of these concerns.\\n\\nGitLab Values Quiz\\nAnyone with a GitLab account can access the GitLab Values Quiz. To participate in the quiz, you will need to complete this learning course in Level Up. If you have questions, please reach out to our L&D team at learning@gitlab.com.\\n\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"simple.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "## load,chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.comNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.comNiki Parmar‚àó\\nGoogle Research\\nnikip@google.comJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.comAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'attention.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.comNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.comNiki Parmar‚àó\\nGoogle Research\\nnikip@google.comJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.comAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='mechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='best models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023', metadata={'source': 'attention.pdf', 'page': 0}),\n",
       " Document(page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='sequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='In this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='Here, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2', metadata={'source': 'attention.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .', metadata={'source': 'attention.pdf', 'page': 2}),\n",
       " Document(page_content='itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,', metadata={'source': 'attention.pdf', 'page': 2}),\n",
       " Document(page_content='3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3', metadata={'source': 'attention.pdf', 'page': 2}),\n",
       " Document(page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n‚àödk)V (1)', metadata={'source': 'attention.pdf', 'page': 3}),\n",
       " Document(page_content='into a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n‚àödk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1‚àödk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has', metadata={'source': 'attention.pdf', 'page': 3}),\n",
       " Document(page_content='dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1‚àödk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q¬∑k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4', metadata={'source': 'attention.pdf', 'page': 3}),\n",
       " Document(page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni‚ààRdmodel√ódk,WK\\ni‚ààRdmodel√ódk,WV\\ni‚ààRdmodel√ódv\\nandWO‚ààRhdv√ódmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,', metadata={'source': 'attention.pdf', 'page': 4}),\n",
       " Document(page_content='The Transformer uses multi-head attention in three different ways:\\n‚Ä¢In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward', metadata={'source': 'attention.pdf', 'page': 4}),\n",
       " Document(page_content='encoder.\\n‚Ä¢Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters', metadata={'source': 'attention.pdf', 'page': 4}),\n",
       " Document(page_content='FFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by‚àödmodel.\\n5', metadata={'source': 'attention.pdf', 'page': 4}),\n",
       " Document(page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2¬∑d) O(1) O(1)\\nRecurrent O(n¬∑d2) O(n) O(n)\\nConvolutional O(k¬∑n¬∑d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r¬∑n¬∑d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel', metadata={'source': 'attention.pdf', 'page': 5}),\n",
       " Document(page_content='tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄto10000 ¬∑2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two', metadata={'source': 'attention.pdf', 'page': 5}),\n",
       " Document(page_content='PEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi‚ààRd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.', metadata={'source': 'attention.pdf', 'page': 5}),\n",
       " Document(page_content='One is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of', metadata={'source': 'attention.pdf', 'page': 5}),\n",
       " Document(page_content='executed operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6', metadata={'source': 'attention.pdf', 'page': 5}),\n",
       " Document(page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than', metadata={'source': 'attention.pdf', 'page': 6}),\n",
       " Document(page_content='orO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k¬∑n¬∑d+n¬∑d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching', metadata={'source': 'attention.pdf', 'page': 6}),\n",
       " Document(page_content='and semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We', metadata={'source': 'attention.pdf', 'page': 6}),\n",
       " Document(page_content='We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with Œ≤1= 0.9,Œ≤2= 0.98andœµ= 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d‚àí0.5\\nmodel¬∑min(step_num‚àí0.5, step _num¬∑warmup _steps‚àí1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7', metadata={'source': 'attention.pdf', 'page': 6}),\n",
       " Document(page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0¬∑1020\\nGNMT + RL [38] 24.6 39.92 2.3¬∑10191.4¬∑1020\\nConvS2S [9] 25.16 40.46 9.6¬∑10181.5¬∑1020\\nMoE [32] 26.03 40.56 2.0¬∑10191.2¬∑1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0¬∑1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8¬∑10201.1¬∑1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7¬∑10191.2¬∑1021\\nTransformer (base model) 27.3 38.1 3.3¬∑1018\\nTransformer (big) 28.4 41.8 2.3¬∑1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.', metadata={'source': 'attention.pdf', 'page': 7}),\n",
       " Document(page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value œµls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,', metadata={'source': 'attention.pdf', 'page': 7}),\n",
       " Document(page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty Œ±= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model', metadata={'source': 'attention.pdf', 'page': 7}),\n",
       " Document(page_content='inference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8', metadata={'source': 'attention.pdf', 'page': 7}),\n",
       " Document(page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop œµlstrain PPL BLEU params\\nsteps (dev) (dev) √ó106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no', metadata={'source': 'attention.pdf', 'page': 8}),\n",
       " Document(page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our', metadata={'source': 'attention.pdf', 'page': 8}),\n",
       " Document(page_content='bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences', metadata={'source': 'attention.pdf', 'page': 8}),\n",
       " Document(page_content='Penn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9', metadata={'source': 'attention.pdf', 'page': 8}),\n",
       " Document(page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andŒ±= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-', metadata={'source': 'attention.pdf', 'page': 9}),\n",
       " Document(page_content='for both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best', metadata={'source': 'attention.pdf', 'page': 9}),\n",
       " Document(page_content='on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint', metadata={'source': 'attention.pdf', 'page': 9}),\n",
       " Document(page_content='comments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10', metadata={'source': 'attention.pdf', 'page': 9}),\n",
       " Document(page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.', metadata={'source': 'attention.pdf', 'page': 10}),\n",
       " Document(page_content='tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832‚Äì841. ACL, August 2009.', metadata={'source': 'attention.pdf', 'page': 10}),\n",
       " Document(page_content='across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.', metadata={'source': 'attention.pdf', 'page': 10}),\n",
       " Document(page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11', metadata={'source': 'attention.pdf', 'page': 10}),\n",
       " Document(page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433‚Äì440. ACL, July\\n2006.', metadata={'source': 'attention.pdf', 'page': 11}),\n",
       " Document(page_content='and interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929‚Äì1958, 2014.', metadata={'source': 'attention.pdf', 'page': 11}),\n",
       " Document(page_content='nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.', metadata={'source': 'attention.pdf', 'page': 11}),\n",
       " Document(page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434‚Äì443. ACL, August 2013.\\n12', metadata={'source': 'attention.pdf', 'page': 11}),\n",
       " Document(page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13', metadata={'source': 'attention.pdf', 'page': 12}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14', metadata={'source': 'attention.pdf', 'page': 13}),\n",
       " Document(page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15', metadata={'source': 'attention.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module langchain_community.embeddings has no attribute ollama",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embeddings\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m----> 5\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(text_documents,embedding\u001b[38;5;241m=\u001b[39m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mollama\u001b[49m\u001b[38;5;241m.\u001b[39mOllamaEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnomic-embed-text\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/PycharmProjects/SeamlessStart/venv/lib/python3.12/site-packages/langchain_community/embeddings/__init__.py:412\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    410\u001b[0m     module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(_module_lookup[name])\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module langchain_community.embeddings has no attribute ollama"
     ]
    }
   ],
   "source": [
    "## Vector Embedding And Vector Store\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(text_documents,embedding=embeddings.ollama.OllamaEmbeddings(model='llama3.1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are the authors of attention is all you need?\"\n",
    "retireved_results=db.similarity_search(query)\n",
    "print(retireved_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FAISS Vector Database\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:15], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
